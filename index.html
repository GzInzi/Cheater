<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cheater</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@1.3.1/dist/face-api.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/handtrackjs/dist/handtrack.min.js"> </script>
    <style>
      body {
        background-color: #f0f0f0;
      }
      .iphone-glass-effect {
        background-color: rgba(255, 255, 255, 0.4);
        backdrop-filter: blur(10px) saturate(180%);
        -webkit-backdrop-filter: blur(10px) saturate(180%);
        border: 1px solid rgba(209, 213, 219, 0.3);
      }
      .answer-highlight {
        background-color: rgba(59, 130, 246, 0.2); /* blue-500 with transparency */
      }
      /* Ensure no scrolling on the page */
      html, body, #root {
        height: 100%;
        overflow: hidden;
      }
      .video-container {
        position: relative;
        width: 100%;
        max-width: 150px; /* Smaller video container */
        margin: 0 auto;
      }
      .video-container video {
        transform: scaleX(-1);
      }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        // Helper function to convert base64 to ArrayBuffer
        const base64ToArrayBuffer = (base64) => {
            const binaryString = window.atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        };

        // Helper function to create a WAV blob from PCM data
        const pcmToWav = (pcmData, sampleRate) => {
            const pcm16 = new Int16Array(pcmData);
            const buffer = new ArrayBuffer(44 + pcm16.length * 2);
            const view = new DataView(buffer);

            // WAV header
            // RIFF chunk
            view.setUint32(0, 0x52494646, false); // "RIFF"
            view.setUint32(4, 36 + pcm16.length * 2, true);
            view.setUint32(8, 0x57415645, false); // "WAVE"

            // fmt chunk
            view.setUint32(12, 0x666d7420, false); // "fmt "
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // format = 1 for PCM
            view.setUint16(22, 1, true); // mono
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true); // byte rate
            view.setUint16(32, 2, true); // block align
            view.setUint16(34, 16, true); // bits per sample

            // data chunk
            view.setUint32(36, 0x64617461, false); // "data"
            view.setUint32(40, pcm16.length * 2, true);

            // write the PCM data
            for (let i = 0; i < pcm16.length; i++) {
                view.setInt16(44 + i * 2, pcm16[i], true);
            }

            return new Blob([view], { type: 'audio/wav' });
        };

        // Main App component
        function App() {
          const [isListening, setIsListening] = useState(false);
          const [recognition, setRecognition] = useState(null);
          const [speechText, setSpeechText] = useState('');
          const [shortAnswer, setShortAnswer] = useState('');
          const [fullAnswer, setFullAnswer] = useState('');
          const [isAiProcessing, setIsAiProcessing] = useState(false);
          const [isSpeaking, setIsSpeaking] = useState(false);
          const [error, setError] = useState('');
          const [isReady, setIsReady] = useState(false);
          const [apiKey, setApiKey] = useState("");
          const [showApiModal, setShowApiModal] = useState(false);
          const [saveApiKey, setSaveApiKey] = useState(false);
          const audioRef = useRef(new Audio());
          const videoRef = useRef();
          const [modelsLoaded, setModelsLoaded] = useState(false);
          const speechTextRef = useRef('');
          
          const [eyesClosedCount, setEyesClosedCount] = useState(0);
          const [handModel, setHandModel] = useState(null);
          const [lastHandState, setLastHandState] = useState(null);
          const [lastTapTime, setLastTapTime] = useState(0);

          // Load API key from localStorage on component mount
          useEffect(() => {
            const savedKey = localStorage.getItem('geminiApiKey');
            if (savedKey) {
              setApiKey(savedKey);
            }
          }, []);

          // Set up the SpeechRecognition API
          useEffect(() => {
            if ('webkitSpeechRecognition' in window) {
              const newRecognition = new window.webkitSpeechRecognition();
              newRecognition.continuous = false;
              newRecognition.lang = 'en-IN';
              newRecognition.interimResults = true;
              
              newRecognition.onstart = () => {
                setIsListening(true);
                setError('');
              };

              newRecognition.onresult = (event) => {
                let finalTranscript = '';
                let interimTranscript = '';
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                  if (event.results[i].isFinal) {
                    finalTranscript += event.results[i][0].transcript;
                  } else {
                    interimTranscript += event.results[i][0].transcript;
                  }
                }
                setSpeechText(finalTranscript || interimTranscript);
              };

              newRecognition.onerror = (event) => {
                setIsListening(false);
                if (event.error === 'not-allowed') {
                  setError("Microphone access was denied. Please enable it in your browser settings.");
                } else {
                  setError(`Speech recognition error: ${event.error}`);
                }
                console.error('Speech recognition error:', event.error);
              };
              
              newRecognition.onend = () => {
                setIsListening(false);
                if (speechTextRef.current.trim() !== '') {
                    // Automatically trigger AI processing after a pause in speech
                    handleAskAI(speechTextRef.current, 'short');
                } else {
                    // Automatically restart listening if nothing was captured
                    recognition.start();
                }
              };
              
              setRecognition(newRecognition);
              setIsReady(true);
            } else {
              setError("Speech Recognition is not supported in this browser.");
            }
          }, []);

          // Update speechTextRef whenever speechText changes
          useEffect(() => {
            speechTextRef.current = speechText;
          }, [speechText]);


          // Load face-api.js and Handtrack.js models and start camera
          useEffect(() => {
            const loadModels = async () => {
              if (window.faceapi) {
                const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@1.3.1/models';
                await faceapi.nets.tinyFaceDetector.load(MODEL_URL);
                await faceapi.nets.faceLandmark68Net.load(MODEL_URL);
              }
              if (window.handTrack) {
                const modelParams = {
                  flip: true,
                  maxNumBoxes: 1,
                  scoreThreshold: 0.7,
                };
                const model = await handTrack.load(modelParams);
                setHandModel(model);
              }
              setModelsLoaded(true);
              startCamera();
            };
            loadModels();
          }, []);

          // Start webcam stream
          const startCamera = () => {
            navigator.mediaDevices.getUserMedia({ video: true })
              .then(stream => {
                videoRef.current.srcObject = stream;
              })
              .catch(err => {
                console.error("Camera access denied:", err);
                setError("Camera access denied. Eye and hand tracking are disabled.");
              });
          };

          // Eye, Hand, and Hand-to-Face tracking logic
          useEffect(() => {
            if (videoRef.current && modelsLoaded) {
              const detectFeatures = setInterval(async () => {
                const faceDetections = await faceapi.detectSingleFace(
                  videoRef.current,
                  new faceapi.TinyFaceDetectorOptions()
                ).withFaceLandmarks();
                
                // Eye tracking logic (blink to restart)
                if (faceDetections) {
                  const leftEye = faceDetections.landmarks.getLeftEye();
                  const rightEye = faceDetections.landmarks.getRightEye();

                  const leftEyeClosed = faceapi.euclideanDistance(leftEye[1], leftEye[4]) < 5 && faceapi.euclideanDistance(leftEye[2], leftEye[3]) < 5;
                  const rightEyeClosed = faceapi.euclideanDistance(rightEye[1], rightEye[4]) < 5 && faceapi.euclideanDistance(rightEye[2], rightEye[3]) < 5;

                  if (leftEyeClosed && rightEyeClosed) {
                    setEyesClosedCount(prev => prev + 1);
                  } else {
                    setEyesClosedCount(0);
                  }
                }

                // Hand gesture tracking logic
                if (handModel) {
                  const predictions = await handModel.detect(videoRef.current);
                  if (predictions && predictions.length > 0) {
                    const gesture = predictions[0].label;
                    const handBox = predictions[0].bbox;
                    
                    // Double-tap logic
                    if (lastHandState === 'closed' && gesture === 'open') {
                      const currentTime = Date.now();
                      if (currentTime - lastTapTime < 500) { // Check if second tap is within 500ms
                          toggleListening();
                          setLastTapTime(0); // Reset timer
                      } else {
                          setLastTapTime(currentTime);
                      }
                    }
                    setLastHandState(gesture);

                    // Hand-to-face detection (forehead touch)
                    if (faceDetections) {
                      const faceBox = faceDetections.detection.box;

                      const handNearForehead = 
                        handBox[0] < faceBox.x + faceBox.width &&
                        handBox[0] + handBox[2] > faceBox.x &&
                        handBox[1] < faceBox.y + faceBox.height / 2 &&
                        handBox[1] + handBox[3] > faceBox.y;

                      if (isListening && handNearForehead) {
                          toggleListening();
                      }
                    }
                  } else {
                    setLastHandState(null);
                  }
                }
              }, 100); // Check every 100ms
              
              return () => clearInterval(detectFeatures);
            }
          }, [isListening, modelsLoaded, handModel, lastHandState, lastTapTime]);
          
          // Trigger action when eyes are closed for a short period
          useEffect(() => {
            if (eyesClosedCount >= 3 && eyesClosedCount <= 9) {
                if (isListening && !speechText) {
                    recognition.stop();
                    setSpeechText('');
                }
            }
          }, [eyesClosedCount, isListening, recognition, speechText]);
          
          // Trigger stop listening when eyes are closed for a period
          useEffect(() => {
            if (eyesClosedCount > 15) {
              if (isListening) {
                toggleListening();
              }
            }
          }, [eyesClosedCount]);


          // Function to start or stop listening
          const toggleListening = () => {
            if (!recognition) {
              setError("Speech Recognition is not available.");
              return;
            }
            if (isListening) {
              recognition.stop();
            } else {
              setSpeechText('');
              setShortAnswer('');
              setFullAnswer('');
              setError('');
              recognition.start();
            }
          };
          
          // Function to call the Gemini API for a short or full answer
          const handleAskAI = async (text, responseType) => {
            if (!text) return;
            let keyToUse = apiKey;
            if (!keyToUse) {
              keyToUse = localStorage.getItem('geminiApiKey');
            }
            
            if (!keyToUse) {
              setShowApiModal(true);
              return;
            }

            setIsAiProcessing(true);

            let prompt;
            // Updated prompt to make the AI better at interpreting broken or imperfect text
            if (responseType === 'short') {
              prompt = `The following text is from a speech-to-text service. It may contain errors. Your task is to first interpret the user's intended question and then provide a direct, concise, and single-sentence answer. The user's text is: "${text}"`;
            } else {
              prompt = `The following text is from a speech-to-text service. It may contain errors. Your task is to first interpret the user's intended question and then provide a detailed, comprehensive, and well-structured answer. The user's text is: "${text}"`;
            }

            try {
              const payload = {
                contents: [{
                  role: "user",
                  parts: [{ text: prompt }]
                }]
              };

              const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${keyToUse}`;

              const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });
              
              if (!response.ok) {
                throw new Error(`API error: ${response.statusText}`);
              }

              const result = await response.json();
              
              if (result.candidates && result.candidates.length > 0 &&
                  result.candidates[0].content && result.candidates[0].content.parts &&
                  result.candidates[0].content.parts.length > 0) {
                const generatedText = result.candidates[0].content.parts[0].text;
                if (responseType === 'short') {
                  setShortAnswer(generatedText);
                  setFullAnswer('');
                } else {
                  setFullAnswer(generatedText);
                }
              } else {
                const errorMessage = "Sorry, I couldn't generate a response. Please try again.";
                if (responseType === 'short') {
                  setShortAnswer(errorMessage);
                } else {
                  setFullAnswer(errorMessage);
                }
              }
            } catch (error) {
              console.error("Error asking AI:", error);
              const errorMessage = "An error occurred while fetching the answer.";
              if (responseType === 'short') {
                setShortAnswer(errorMessage);
              } else {
                setFullAnswer(errorMessage);
              }
            } finally {
              setIsAiProcessing(false);
            }
          };

          // Function to handle Text-to-Speech
          const handleSpeakAnswer = async (textToSpeak) => {
            if (!textToSpeak) return;
            setIsSpeaking(true);

            let keyToUse = apiKey;
            if (!keyToUse) {
              keyToUse = localStorage.getItem('geminiApiKey');
            }
            
            if (!keyToUse) {
              setShowApiModal(true);
              setIsSpeaking(false);
              return;
            }

            try {
              const payload = {
                contents: [{ parts: [{ text: textToSpeak }] }],
                generationConfig: {
                    responseModalities: ["AUDIO"],
                    speechConfig: {
                        voiceConfig: {
                            prebuiltVoiceConfig: { voiceName: "Puck" }
                        }
                    }
                },
                model: "gemini-2.5-flash-preview-tts"
              };
              
              const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${keyToUse}`;

              const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });

              const result = await response.json();
              const part = result?.candidates?.[0]?.content?.parts?.[0];
              const audioData = part?.inlineData?.data;
              const mimeType = part?.inlineData?.mimeType;

              if (audioData && mimeType && mimeType.startsWith("audio/")) {
                const sampleRate = parseInt(mimeType.match(/rate=(\d+)/)[1], 10);
                const pcmData = base64ToArrayBuffer(audioData);
                const wavBlob = pcmToWav(pcmData, sampleRate);
                const audioUrl = URL.createObjectURL(wavBlob);

                audioRef.current.src = audioUrl;
                audioRef.current.play();

                audioRef.current.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    setIsSpeaking(false);
                };
              } else {
                setError("Error converting text to speech.");
                setIsSpeaking(false);
              }
            } catch (error) {
              console.error("TTS error:", error);
              setError("An error occurred with the text-to-speech function.");
              setIsSpeaking(false);
            }
          };
          
          return (
            <div className="h-full bg-gray-200 font-sans text-gray-800 p-4 flex flex-col items-center justify-center">
              <div className="iphone-glass-effect p-6 rounded-3xl shadow-lg w-full max-w-sm flex flex-col items-center space-y-4 text-center">
                
                {/* Cheater */}
                <div className="mb-2">
                  <h1 className="text-2xl font-bold text-gray-800">Cheater</h1>
                  <p className="text-gray-600 text-xs">Use gestures to interact.</p>
                </div>

                {/* Video container for hand gesture and eye tracking */}
                <div className="video-container">
                  <video
                    ref={videoRef}
                    autoPlay
                    muted
                    className="w-full h-auto rounded-xl shadow-lg border border-gray-300"
                  ></video>
                </div>

                {/* Microphone Button */}
                <button
                  onClick={toggleListening}
                  disabled={!isReady}
                  className={`w-20 h-20 rounded-full transition-all duration-300 shadow-lg 
                  ${isListening ? 'bg-red-500 transform scale-110 ring-4 ring-red-300' : 'bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400'}`}
                >
                  <svg className={`mx-auto ${isListening ? 'animate-pulse' : ''}`} width="48" height="48" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12 14C14.21 14 16 12.21 16 10V4C16 1.79 14.21 0 12 0C9.79 0 8 1.79 8 4V10C8 12.21 9.79 14 12 14ZM10 4C10 2.9 10.9 2 12 2C13.1 2 14 2.9 14 4V10C14 11.1 13.1 12 12 12C10.9 12 10 11.1 10 10V4ZM12 18C15.31 18 18.06 15.69 18.06 12.5H20.06C20.06 16.3 17.15 19.45 13.5 19.96V23H10.5V19.96C6.85 19.45 3.94 16.3 3.94 12.5H5.94C5.94 15.69 8.69 18 12 18Z" fill="currentColor"/>
                  </svg>
                </button>

                {/* Status Messages */}
                <div className="h-4">
                  {error && <p className="text-red-500 text-sm">{error}</p>}
                  {isListening && <p className="text-gray-800 text-sm animate-pulse">Listening...</p>}
                  {isAiProcessing && !isListening && <p className="text-gray-800 text-sm animate-pulse">Thinking...</p>}
                </div>

                {/* Output Section */}
                <div className="space-y-4 w-full">
                  <div className="iphone-glass-effect p-4 rounded-xl text-left h-20 overflow-hidden border border-gray-300">
                    <h2 className="text-base font-bold mb-1 text-gray-800">Your Question:</h2>
                    <p className="text-sm text-gray-600">{speechText || <span className="italic text-gray-400">Your question will appear here.</span>}</p>
                  </div>
                  
                  {shortAnswer && (
                    <div className="space-y-4">
                        <div className={`relative iphone-glass-effect p-4 rounded-xl text-left h-24 overflow-auto border border-gray-300 ${shortAnswer && 'answer-highlight'}`}>
                          <h2 className="text-base font-bold mb-1 text-gray-800">AI Answer:</h2>
                          <p className="text-xl font-bold text-gray-600">{shortAnswer}</p>
                          <button
                            onClick={() => handleSpeakAnswer(shortAnswer)}
                            disabled={isSpeaking}
                            className="absolute bottom-2 right-2 bg-gray-300 hover:bg-gray-400 disabled:bg-gray-400 text-gray-800 p-1 rounded-full transition duration-200"
                            aria-label="Read answer aloud"
                          >
                            {isSpeaking ? (
                              <svg className="animate-spin h-4 w-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                                <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                                <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                              </svg>
                            ) : (
                              <svg xmlns="http://www.w3.org/2000/svg" className="h-4 w-4" viewBox="0 0 24 24" fill="currentColor">
                                <path d="M14.5 12c0 .9-.7 1.7-1.5 1.9v4.2c2.8-.5 5-3 5-6.1s-2.2-5.6-5-6.1v4.2c.8.2 1.5 1 1.5 1.9zm-2.5 0V6.1c-2.8.5-5 3-5 6.1s2.2 5.6 5 6.1v-4.2c-.8-.2-1.5-1-1.5-1.9zm-3.5 0H5.5c0 3.8 2.5 7.1 6 7.9V21h1v-1.9c3.5-.9 6-4.2 6-7.9h-1c0 3.1-2.2 5.6-5 6.1v-4.2c.8.2 1.5 1 1.5 1.9v-4.2c-2.8.5-5 3-5 6.1v4.2c.8.2 1.5 1 1.5 1.9z"/>
                              </svg>
                            )}
                          </button>
                        </div>
                        {!fullAnswer && (
                          <button
                            onClick={() => handleAskAI(speechText, 'full')}
                            disabled={isAiProcessing}
                            className="w-full p-2 bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400 text-white font-bold rounded-xl transition duration-200"
                          >
                            {isAiProcessing ? "Loading..." : "View Full Answer"}
                          </button>
                        )}
                        {fullAnswer && (
                          <div className="iphone-glass-effect p-4 rounded-xl text-left max-h-32 overflow-auto border border-gray-300">
                              <h2 className="text-base font-bold mb-1 text-gray-800">Full Answer:</h2>
                              <p className="text-sm text-gray-600 whitespace-pre-wrap">{fullAnswer}</p>
                          </div>
                        )}
                    </div>
                  )}
                </div>
              </div>
              {showApiModal && (
                <div className="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center p-4">
                  <div className="bg-white p-6 rounded-xl shadow-lg w-full max-w-md">
                    <h2 className="text-xl font-bold text-gray-800 mb-4">Enter your API Key</h2>
                    <p className="text-gray-600 mb-4">To use the AI functionality, you need to provide your Gemini API key. This will not be saved anywhere.</p>
                    <input
                      type="password"
                      className="w-full p-2 border border-gray-300 rounded-md mb-4 text-gray-800"
                      placeholder="Enter your API key here..."
                      value={apiKey}
                      onChange={(e) => setApiKey(e.target.value)}
                    />
                    <div className="flex items-center space-x-2 mb-4">
                      <input
                        type="checkbox"
                        id="save-key-checkbox"
                        checked={saveApiKey}
                        onChange={(e) => setSaveApiKey(e.target.checked)}
                        className="h-4 w-4 text-blue-600 border-gray-300 rounded"
                      />
                      <label htmlFor="save-key-checkbox" className="text-gray-600">Save API Key</label>
                    </div>
                    <div className="flex justify-end space-x-2">
                      <button
                        onClick={() => setShowApiModal(false)}
                        className="bg-gray-200 hover:bg-gray-300 text-gray-800 font-bold py-2 px-4 rounded-xl transition duration-200"
                      >
                        Cancel
                      </button>
                      <button
                        onClick={() => {
                          if (saveApiKey) {
                            localStorage.setItem('geminiApiKey', apiKey);
                          } else {
                            localStorage.removeItem('geminiApiKey');
                          }
                          setShowApiModal(false);
                          if (speechText) {
                            handleAskAI(speechText, 'short');
                          }
                        }}
                        disabled={!apiKey}
                        className="bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400 text-white font-bold py-2 px-4 rounded-xl transition duration-200"
                      >
                        Save and Use
                      </button>
                    </div>
                  </div>
                </div>
              )}
            </div>
          );
        }

        // Render the App component into the root element
        const rootElement = document.getElementById('root');
        ReactDOM.render(<App />, rootElement);
    </script>
</body>
</html>
