<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cheater</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@1.3.1/dist/face-api.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/handtrackjs/dist/handtrack.min.js"> </script>
    <style>
      body {
        background-color: #f0f0f0;
      }
      .iphone-glass-effect {
        background-color: rgba(255, 255, 255, 0.4);
        backdrop-filter: blur(10px) saturate(180%);
        -webkit-backdrop-filter: blur(10px) saturate(180%);
        border: 1px solid rgba(209, 213, 219, 0.3);
      }
      .answer-highlight {
        background-color: rgba(59, 130, 246, 0.2); /* blue-500 with transparency */
      }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        // Helper function to convert base64 to ArrayBuffer
        const base64ToArrayBuffer = (base64) => {
            const binaryString = window.atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        };

        // Helper function to create a WAV blob from PCM data
        const pcmToWav = (pcmData, sampleRate) => {
            const pcm16 = new Int16Array(pcmData);
            const buffer = new ArrayBuffer(44 + pcm16.length * 2);
            const view = new DataView(buffer);

            // WAV header
            // RIFF chunk
            view.setUint32(0, 0x52494646, false); // "RIFF"
            view.setUint32(4, 36 + pcm16.length * 2, true);
            view.setUint32(8, 0x57415645, false); // "WAVE"

            // fmt chunk
            view.setUint32(12, 0x666d7420, false); // "fmt "
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // format = 1 for PCM
            view.setUint16(22, 1, true); // mono
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true); // byte rate
            view.setUint16(32, 2, true); // block align
            view.setUint16(34, 16, true); // bits per sample

            // data chunk
            view.setUint32(36, 0x64617461, false); // "data"
            view.setUint32(40, pcm16.length * 2, true);

            // write the PCM data
            for (let i = 0; i < pcm16.length; i++) {
                view.setInt16(44 + i * 2, pcm16[i], true);
            }

            return new Blob([view], { type: 'audio/wav' });
        };

        // Main App component
        function App() {
          const [isListening, setIsListening] = useState(false);
          const [recognition, setRecognition] = useState(null);
          const [speechText, setSpeechText] = useState('');
          const [shortAnswer, setShortAnswer] = useState('');
          const [fullAnswer, setFullAnswer] = useState('');
          const [isAiProcessing, setIsAiProcessing] = useState(false);
          const [isSpeaking, setIsSpeaking] = useState(false);
          const [error, setError] = useState('');
          const [isReady, setIsReady] = useState(false);
          const [apiKey, setApiKey] = useState("");
          const [showApiModal, setShowApiModal] = useState(false);
          const [saveApiKey, setSaveApiKey] = useState(false);
          const audioRef = useRef(new Audio());
          const videoRef = useRef();
          const [modelsLoaded, setModelsLoaded] = useState(false);
          const speechTextRef = useRef('');

          // State for eye and hand tracking
          const [eyesClosedCount, setEyesClosedCount] = useState(0);
          const [handModel, setHandModel] = useState(null);

          // Load API key from localStorage on component mount
          useEffect(() => {
            const savedKey = localStorage.getItem('geminiApiKey');
            if (savedKey) {
              setApiKey(savedKey);
            }
          }, []);

          // Set up the SpeechRecognition API
          useEffect(() => {
            if ('webkitSpeechRecognition' in window) {
              const newRecognition = new window.webkitSpeechRecognition();
              newRecognition.continuous = false;
              newRecognition.lang = 'en-IN';
              newRecognition.interimResults = true;
              
              newRecognition.onstart = () => {
                setIsListening(true);
                setError('');
              };

              newRecognition.onresult = (event) => {
                let finalTranscript = '';
                let interimTranscript = '';
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                  if (event.results[i].isFinal) {
                    finalTranscript += event.results[i][0].transcript;
                  } else {
                    interimTranscript += event.results[i][0].transcript;
                  }
                }
                setSpeechText(finalTranscript || interimTranscript);
              };

              newRecognition.onerror = (event) => {
                setIsListening(false);
                if (event.error === 'not-allowed') {
                  setError("Microphone access was denied. Please enable it in your browser settings.");
                } else {
                  setError(`Speech recognition error: ${event.error}`);
                }
                console.error('Speech recognition error:', event.error);
              };
              
              newRecognition.onend = () => {
                setIsListening(false);
                if (speechTextRef.current.trim() !== '') {
                    handleAskAI(speechTextRef.current, 'short');
                } else {
                    recognition.start();
                }
              };
              
              setRecognition(newRecognition);
              setIsReady(true);
            } else {
              setError("Speech Recognition is not supported in this browser.");
            }
          }, []);

          // Update speechTextRef whenever speechText changes
          useEffect(() => {
            speechTextRef.current = speechText;
          }, [speechText]);


          // Load face-api.js and Handtrack.js models and start camera
          useEffect(() => {
            const loadModels = async () => {
              if (window.faceapi) {
                const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@1.3.1/models';
                await faceapi.nets.tinyFaceDetector.load(MODEL_URL);
                await faceapi.nets.faceLandmark68Net.load(MODEL_URL);
              }
              if (window.handTrack) {
                const modelParams = {
                  flip: true,
                  maxNumBoxes: 1,
                  scoreThreshold: 0.7,
                };
                const model = await handTrack.load(modelParams);
                setHandModel(model);
              }
              setModelsLoaded(true);
              startCamera();
            };
            loadModels();
          }, []);

          // Start webcam stream
          const startCamera = () => {
            navigator.mediaDevices.getUserMedia({ video: true })
              .then(stream => {
                videoRef.current.srcObject = stream;
              })
              .catch(err => {
                console.error("Camera access denied:", err);
                setError("Camera access denied. Eye and hand tracking are disabled.");
              });
          };

          // Eye, Hand, and Hand-to-Face tracking logic
          useEffect(() => {
            if (isListening && videoRef.current && modelsLoaded) {
              const detectFeatures = setInterval(async () => {
                const faceDetections = await faceapi.detectSingleFace(
                  videoRef.current,
                  new faceapi.TinyFaceDetectorOptions()
                ).withFaceLandmarks();
                
                // Eye tracking logic
                if (faceDetections) {
                  const leftEye = faceDetections.landmarks.getLeftEye();
                  const rightEye = faceDetections.landmarks.getRightEye();

                  const leftEyeClosed = faceapi.euclideanDistance(leftEye[1], leftEye[4]) < 5 && faceapi.euclideanDistance(leftEye[2], leftEye[3]) < 5;
                  const rightEyeClosed = faceapi.euclideanDistance(rightEye[1], rightEye[4]) < 5 && faceapi.euclideanDistance(rightEye[2], rightEye[3]) < 5;

                  if (leftEyeClosed && rightEyeClosed) {
                    setEyesClosedCount(prev => prev + 1);
                  } else {
                    setEyesClosedCount(0);
                  }
                }

                // Hand gesture tracking logic
                if (handModel) {
                  const predictions = await handModel.detect(videoRef.current);
                  if (predictions && predictions.length > 0) {
                    const gesture = predictions[0].label;
                    const handBox = predictions[0].bbox;

                    // Hand-to-face detection (forehead touch) - More Sensitive Logic
                    if (faceDetections) {
                      const faceBox = faceDetections.detection.box;

                      // Calculate the center of the hand and the top of the face
                      const handCenterY = handBox[1] + handBox[3] / 2;
                      const foreheadTop = faceBox.y;
                      const foreheadBottom = faceBox.y + faceBox.height / 3;

                      // Check if the hand's center is within the forehead area
                      const isHandNearForehead = 
                        handBox[0] < faceBox.x + faceBox.width &&
                        handBox[0] + handBox[2] > faceBox.x &&
                        handCenterY > foreheadTop &&
                        handCenterY < foreheadBottom;

                      if (isHandNearForehead) {
                          if (isListening) {
                              toggleListening();
                          }
                      }
                    }

                    // Open/Closed hand gesture control
                    if (gesture === 'open') {
                      if (!isListening) {
                        toggleListening();
                      }
                    } else if (gesture === 'closed') {
                      if (isListening) {
                        toggleListening();
                      }
                    }
                  }
                }
              }, 100); // Check every 100ms
              
              return () => clearInterval(detectFeatures);
            }
          }, [isListening, modelsLoaded, handModel]);
          
          // Trigger action when eyes are closed for a short period
          useEffect(() => {
            if (eyesClosedCount >= 3 && eyesClosedCount <= 9) {
                if (isListening && !speechText) {
                    recognition.stop();
                    setSpeechText('');
                }
            }
          }, [eyesClosedCount, isListening, recognition, speechText]);
          
          // Trigger stop listening when eyes are closed for a period
          useEffect(() => {
            if (eyesClosedCount > 15) {
              if (isListening) {
                toggleListening();
              }
            }
          }, [eyesClosedCount]);


          // Function to start or stop listening
          const toggleListening = () => {
            if (!recognition) {
              setError("Speech Recognition is not available.");
              return;
            }
            if (isListening) {
              recognition.stop();
            } else {
              setSpeechText('');
              setShortAnswer('');
              setFullAnswer('');
              setError('');
              recognition.start();
            }
          };
          
          // Function to call the Gemini API for a short or full answer
          const handleAskAI = async (text, responseType) => {
            if (!text) return;
            let keyToUse = apiKey;
            if (!keyToUse) {
              keyToUse = localStorage.getItem('geminiApiKey');
            }
            
            if (!keyToUse) {
              setShowApiModal(true);
              return;
            }

            setIsAiProcessing(true);

            let prompt;
            if (responseType === 'short') {
              prompt = `Act as a research assistant. The user will ask a question. Your primary goal is to find the most current and accurate answer. Your response should be direct, accurate, and extremely concise, in a single sentence if possible. The user's question is: "${text}"`;
            } else {
              prompt = `Act as a research assistant. The user will ask a question. Your primary goal is to find the most current and accurate answer. Your response should be detailed, accurate, and comprehensive, providing a full explanation. The user's question is: "${text}"`;
            }

            try {
              const payload = {
                contents: [{
                  role: "user",
                  parts: [{ text: prompt }]
                }]
              };

              const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${keyToUse}`;

              const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });
              
              if (!response.ok) {
                throw new Error(`API error: ${response.statusText}`);
              }

              const result = await response.json();
              
              if (result.candidates && result.candidates.length > 0 &&
                  result.candidates[0].content && result.candidates[0].content.parts &&
                  result.candidates[0].content.parts.length > 0) {
                const generatedText = result.candidates[0].content.parts[0].text;
                if (responseType === 'short') {
                  setShortAnswer(generatedText);
                  setFullAnswer('');
                } else {
                  setFullAnswer(generatedText);
                }
              } else {
                const errorMessage = "Sorry, I couldn't generate a response. Please try again.";
                if (responseType === 'short') {
                  setShortAnswer(errorMessage);
                } else {
                  setFullAnswer(errorMessage);
                }
              }
            } catch (error) {
              console.error("Error asking AI:", error);
              const errorMessage = "An error occurred while fetching the answer.";
              if (responseType === 'short') {
                setShortAnswer(errorMessage);
              } else {
                setFullAnswer(errorMessage);
              }
            } finally {
              setIsAiProcessing(false);
            }
          };

          // Function to handle Text-to-Speech
          const handleSpeakAnswer = async (textToSpeak) => {
            if (!textToSpeak) return;
            setIsSpeaking(true);

            let keyToUse = apiKey;
            if (!keyToUse) {
              keyToUse = localStorage.getItem('geminiApiKey');
            }
            
            if (!keyToUse) {
              setShowApiModal(true);
              setIsSpeaking(false);
              return;
            }

            try {
              const payload = {
                contents: [{ parts: [{ text: textToSpeak }] }],
                generationConfig: {
                    responseModalities: ["AUDIO"],
                    speechConfig: {
                        voiceConfig: {
                            prebuiltVoiceConfig: { voiceName: "Puck" }
                        }
                    }
                },
                model: "gemini-2.5-flash-preview-tts"
              };
              
              const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${keyToUse}`;

              const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });

              const result = await response.json();
              const part = result?.candidates?.[0]?.content?.parts?.[0];
              const audioData = part?.inlineData?.data;
              const mimeType = part?.inlineData?.mimeType;

              if (audioData && mimeType && mimeType.startsWith("audio/")) {
                const sampleRate = parseInt(mimeType.match(/rate=(\d+)/)[1], 10);
                const pcmData = base64ToArrayBuffer(audioData);
                const wavBlob = pcmToWav(pcmData, sampleRate);
                const audioUrl = URL.createObjectURL(wavBlob);

                audioRef.current.src = audioUrl;
                audioRef.current.play();

                audioRef.current.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    setIsSpeaking(false);
                };
              } else {
                setError("Error converting text to speech.");
                setIsSpeaking(false);
              }
            } catch (error) {
              console.error("TTS error:", error);
              setError("An error occurred with the text-to-speech function.");
              setIsSpeaking(false);
            }
          };
          
          return (
            <div className="min-h-screen bg-gray-200 font-sans text-gray-800 p-8 flex flex-col items-center justify-center">
              <div className="iphone-glass-effect p-6 rounded-3xl shadow-lg w-full max-w-sm space-y-6 border text-center">
                
                {/* Cheater */}
                <div className="mb-4">
                  <h1 className="text-3xl font-bold text-gray-800 mb-2">Cheater</h1>
                  <p className="text-gray-600">Tap the mic, speak your question, and get an instant answer.</p>
                </div>

                {/* Visible video element for eye and hand tracking */}
                <video
                  ref={videoRef}
                  autoPlay
                  muted
                  className="w-full h-auto rounded-xl shadow-lg border border-gray-300"
                ></video>

                {/* Microphone Button */}
                <button
                  onClick={toggleListening}
                  disabled={!isReady}
                  className={`w-28 h-28 rounded-full transition-all duration-300 shadow-lg 
                  ${isListening ? 'bg-red-500 transform scale-110 ring-4 ring-red-300' : 'bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400'}`}
                >
                  <svg className={`mx-auto ${isListening ? 'animate-pulse' : ''}`} width="48" height="48" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12 14C14.21 14 16 12.21 16 10V4C16 1.79 14.21 0 12 0C9.79 0 8 1.79 8 4V10C8 12.21 9.79 14 12 14ZM10 4C10 2.9 10.9 2 12 2C13.1 2 14 2.9 14 4V10C14 11.1 13.1 12 12 12C10.9 12 10 11.1 10 10V4ZM12 18C15.31 18 18.06 15.69 18.06 12.5H20.06C20.06 16.3 17.15 19.45 13.5 19.96V23H10.5V19.96C6.85 19.45 3.94 16.3 3.94 12.5H5.94C5.94 15.69 8.69 18 12 18Z" fill="currentColor"/>
                  </svg>
                </button>

                {/* Status Messages */}
                <div className="h-6">
                  {error && <p className="text-red-500">{error}</p>}
                  {isListening && <p className="text-gray-800 animate-pulse">Listening...</p>}
                  {isAiProcessing && !isListening && <p className="text-gray-800 animate-pulse">Thinking...</p>}
                </div>

                {/* Output Section */}
                <div className="space-y-4">
                  <div className="iphone-glass-effect p-4 rounded-xl text-left min-h-[6rem] max-h-48 overflow-auto border border-gray-300">
                    <h2 className="text-lg font-bold mb-2 text-gray-800">Your Question:</h2>
                    <p className="text-gray-600">{speechText || <span className="italic text-gray-400">Your question will appear here.</span>}</p>
                  </div>
                  
                  {shortAnswer && (
                    <div className="space-y-4">
                        <div className={`relative iphone-glass-effect p-4 rounded-xl text-left border border-gray-300 ${shortAnswer && 'answer-highlight'}`}>
                          <h2 className="text-lg font-bold mb-2 text-gray-800">AI Answer:</h2>
                          <p className="text-gray-600">{shortAnswer}</p>
                          <button
                            onClick={() => handleSpeakAnswer(shortAnswer)}
                            disabled={isSpeaking}
                            className="absolute bottom-4 right-4 bg-gray-300 hover:bg-gray-400 disabled:bg-gray-400 text-gray-800 p-2 rounded-full transition duration-200"
                            aria-label="Read answer aloud"
                          >
                            {isSpeaking ? (
                              <svg className="animate-spin h-5 w-5" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                                <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                                <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                              </svg>
                            ) : (
                              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" viewBox="0 0 24 24" fill="currentColor">
                                <path d="M14.5 12c0 .9-.7 1.7-1.5 1.9v4.2c2.8-.5 5-3 5-6.1s-2.2-5.6-5-6.1v4.2c.8.2 1.5 1 1.5 1.9zm-2.5 0V6.1c-2.8.5-5 3-5 6.1s2.2 5.6 5 6.1v-4.2c-.8-.2-1.5-1-1.5-1.9zm-3.5 0H5.5c0 3.8 2.5 7.1 6 7.9V21h1v-1.9c3.5-.9 6-4.2 6-7.9h-1c0 3.1-2.2 5.6-5 6.1v-4.2c.8.2 1.5 1 1.5 1.9v-4.2c-2.8.5-5 3-5 6.1v4.2c.8.2 1.5 1 1.5 1.9z"/>
                              </svg>
                            )}
                          </button>
                        </div>
                        {!fullAnswer && (
                          <button
                            onClick={() => handleAskAI(speechText, 'full')}
                            disabled={isAiProcessing}
                            className="w-full p-2 bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400 text-white font-bold rounded-xl transition duration-200"
                          >
                            {isAiProcessing ? "Loading..." : "View Full Answer"}
                          </button>
                        )}
                        {fullAnswer && (
                          <div className="iphone-glass-effect p-4 rounded-xl text-left border border-gray-300">
                              <h2 className="text-lg font-bold mb-2 text-gray-800">Full Answer:</h2>
                              <p className="text-gray-600 whitespace-pre-wrap">{fullAnswer}</p>
                          </div>
                        )}
                    </div>
                  )}
                </div>
              </div>
              {showApiModal && (
                <div className="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center p-4">
                  <div className="bg-white p-6 rounded-xl shadow-lg w-full max-w-md">
                    <h2 className="text-xl font-bold text-gray-800 mb-4">Enter your API Key</h2>
                    <p className="text-gray-600 mb-4">To use the AI functionality, you need to provide your Gemini API key. This will not be saved anywhere.</p>
                    <input
                      type="password"
                      className="w-full p-2 border border-gray-300 rounded-md mb-4 text-gray-800"
                      placeholder="Enter your API key here..."
                      value={apiKey}
                      onChange={(e) => setApiKey(e.target.value)}
                    />
                    <div className="flex items-center space-x-2 mb-4">
                      <input
                        type="checkbox"
                        id="save-key-checkbox"
                        checked={saveApiKey}
                        onChange={(e) => setSaveApiKey(e.target.checked)}
                        className="h-4 w-4 text-blue-600 border-gray-300 rounded"
                      />
                      <label htmlFor="save-key-checkbox" className="text-gray-600">Save API Key</label>
                    </div>
                    <div className="flex justify-end space-x-2">
                      <button
                        onClick={() => setShowApiModal(false)}
                        className="bg-gray-200 hover:bg-gray-300 text-gray-800 font-bold py-2 px-4 rounded-xl transition duration-200"
                      >
                        Cancel
                      </button>
                      <button
                        onClick={() => {
                          if (saveApiKey) {
                            localStorage.setItem('geminiApiKey', apiKey);
                          } else {
                            localStorage.removeItem('geminiApiKey');
                          }
                          setShowApiModal(false);
                          if (speechText) {
                            handleAskAI(speechText, 'short');
                          }
                        }}
                        disabled={!apiKey}
                        className="bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400 text-white font-bold py-2 px-4 rounded-xl transition duration-200"
                      >
                        Save and Use
                      </button>
                    </div>
                  </div>
                </div>
              )}
            </div>
          );
        }

        // Render the App component into the root element
        const rootElement = document.getElementById('root');
        ReactDOM.render(<App />, rootElement);
    </script>
</body>
</html>
