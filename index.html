<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI Assistant</title>
    <!-- Tailwind CSS CDN for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- React and ReactDOM from CDN -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <!-- Babel for JSX compilation in the browser -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <style>
      body {
        background-color: #f0f0f0;
      }
      .iphone-glass-effect {
        background-color: rgba(255, 255, 255, 0.4);
        backdrop-filter: blur(10px) saturate(180%);
        -webkit-backdrop-filter: blur(10px) saturate(180%);
        border: 1px solid rgba(209, 213, 219, 0.3);
      }
      .answer-highlight {
        background-color: rgba(59, 130, 246, 0.2); /* blue-500 with transparency */
      }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect } = React;

        // Main App component
        function App() {
          const [isListening, setIsListening] = useState(false);
          const [recognition, setRecognition] = useState(null);
          const [speechText, setSpeechText] = useState('');
          const [answer, setAnswer] = useState('');
          const [isAiProcessing, setIsAiProcessing] = useState(false);
          const [error, setError] = useState('');
          const [isReady, setIsReady] = useState(false);

          // Set up the SpeechRecognition API
          useEffect(() => {
            if ('webkitSpeechRecognition' in window) {
              const newRecognition = new window.webkitSpeechRecognition();
              newRecognition.continuous = true; // Stay active to catch slow speech
              newRecognition.lang = 'en-US';
              newRecognition.interimResults = true; // Show results as they are being spoken

              newRecognition.onstart = () => {
                setIsListening(true);
                setError('');
              };

              newRecognition.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                  if (event.results[i].isFinal) {
                    finalTranscript += event.results[i][0].transcript;
                  } else {
                    interimTranscript += event.results[i][0].transcript;
                  }
                }
                setSpeechText(finalTranscript || interimTranscript);
                if (finalTranscript) {
                  handleAskAI(finalTranscript);
                }
              };

              newRecognition.onerror = (event) => {
                setIsListening(false);
                if (event.error === 'not-allowed') {
                  setError("Microphone access was denied. Please enable it in your browser settings.");
                } else {
                  setError(`Speech recognition error: ${event.error}`);
                }
                console.error('Speech recognition error:', event.error);
              };

              newRecognition.onend = () => {
                setIsListening(false);
              };
              
              setRecognition(newRecognition);
              setIsReady(true);
            } else {
              setError("Speech Recognition is not supported in this browser.");
            }
          }, []);

          // Function to start or stop listening
          const toggleListening = () => {
            if (!recognition) {
              setError("Speech Recognition is not available.");
              return;
            }
            if (isListening) {
              recognition.stop();
            } else {
              setSpeechText('');
              setAnswer('');
              setError('');
              recognition.start();
            }
          };

          // Function to call the Gemini API for an answer
          const handleAskAI = async (text) => {
            if (!text) return;
            setIsAiProcessing(true);
            setAnswer('');

            try {
              // The prompt is now structured to give a very concise answer
              const prompt = `Based on the following spoken question, provide a short, accurate answer. If the question contains options, please choose the correct option. Your response should contain ONLY the correct answer, nothing else. The input is: "${text}"`;
              
              const payload = {
                contents: [{
                  role: "user",
                  parts: [{ text: prompt }]
                }]
              };

              const apiKey = "";
              const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

              const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });
              
              if (!response.ok) {
                throw new Error(`API error: ${response.statusText}`);
              }

              const result = await response.json();
              
              if (result.candidates && result.candidates.length > 0 &&
                  result.candidates[0].content && result.candidates[0].content.parts &&
                  result.candidates[0].content.parts.length > 0) {
                const generatedText = result.candidates[0].content.parts[0].text;
                setAnswer(generatedText);
              } else {
                setAnswer("Sorry, I couldn't generate a response. Please try again.");
              }
            } catch (error) {
              console.error("Error asking AI:", error);
              setAnswer("An error occurred while fetching the answer.");
            } finally {
              setIsAiProcessing(false);
            }
          };
          
          return (
            <div className="min-h-screen bg-gray-200 font-sans text-gray-800 p-8 flex flex-col items-center justify-center">
              <div className="iphone-glass-effect p-6 rounded-3xl shadow-lg w-full max-w-sm space-y-6 border text-center">
                
                {/* Cheater */}
                <div className="mb-4">
                  <h1 className="text-3xl font-bold text-gray-800 mb-2">Cheater</h1>
                  <p className="text-gray-600">Tap the mic, speak your question, and get an instant answer.</p>
                </div>

                {/* Microphone Button */}
                <button
                  onClick={toggleListening}
                  disabled={!isReady || isListening}
                  className={`w-28 h-28 rounded-full transition-all duration-300 shadow-lg 
                  ${isListening ? 'bg-red-500 transform scale-110 ring-4 ring-red-300' : 'bg-blue-500 hover:bg-blue-600 disabled:bg-gray-400'}`}
                >
                  <svg className={`mx-auto ${isListening ? 'animate-pulse' : ''}`} width="48" height="48" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12 14C14.21 14 16 12.21 16 10V4C16 1.79 14.21 0 12 0C9.79 0 8 1.79 8 4V10C8 12.21 9.79 14 12 14ZM10 4C10 2.9 10.9 2 12 2C13.1 2 14 2.9 14 4V10C14 11.1 13.1 12 12 12C10.9 12 10 11.1 10 10V4ZM12 18C15.31 18 18.06 15.69 18.06 12.5H20.06C20.06 16.3 17.15 19.45 13.5 19.96V23H10.5V19.96C6.85 19.45 3.94 16.3 3.94 12.5H5.94C5.94 15.69 8.69 18 12 18Z" fill="currentColor"/>
                  </svg>
                </button>

                {/* Status Messages */}
                <div className="h-6">
                  {error && <p className="text-red-500">{error}</p>}
                  {isListening && <p className="text-gray-800 animate-pulse">Listening...</p>}
                  {isAiProcessing && !isListening && <p className="text-gray-800 animate-pulse">Thinking...</p>}
                </div>

                {/* Output Section */}
                <div className="space-y-4">
                  <div className="iphone-glass-effect p-4 rounded-xl text-left min-h-[6rem] max-h-48 overflow-auto border border-gray-300">
                    <h2 className="text-lg font-bold mb-2 text-gray-800">Your Question:</h2>
                    <p className="text-gray-600">{speechText || <span className="italic text-gray-400">Your question will appear here.</span>}</p>
                  </div>

                  <div className={`iphone-glass-effect p-4 rounded-xl text-left min-h-[6rem] max-h-48 overflow-auto border border-gray-300 ${answer && 'answer-highlight'}`}>
                    <h2 className="text-lg font-bold mb-2 text-gray-800">AI Answer:</h2>
                    <p className="text-gray-600">{answer || <span className="italic text-gray-400">The AI's answer will appear here.</span>}</p>
                  </div>
                </div>
              </div>
            </div>
          );
        }

        // Render the App component into the root element
        const rootElement = document.getElementById('root');
        ReactDOM.render(<App />, rootElement);
    </script>
</body>
</html>
