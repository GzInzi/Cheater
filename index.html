<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI Assistant</title>
    <!-- Tailwind CSS CDN for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- React and ReactDOM from CDN -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <!-- Babel for JSX compilation in the browser -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect } = React;

        // Main App component
        function App() {
          const [isListening, setIsListening] = useState(false);
          const [recognition, setRecognition] = useState(null);
          const [speechText, setSpeechText] = useState('');
          const [answer, setAnswer] = useState('');
          const [isAiProcessing, setIsAiProcessing] = useState(false);
          const [error, setError] = useState('');

          // Set up the SpeechRecognition API
          useEffect(() => {
            if ('webkitSpeechRecognition' in window) {
              const newRecognition = new window.webkitSpeechRecognition();
              newRecognition.continuous = false; // Stop after one phrase
              newRecognition.lang = 'en-US';
              newRecognition.interimResults = false;

              newRecognition.onstart = () => {
                setIsListening(true);
                setError('');
              };

              newRecognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                setSpeechText(transcript);
                setIsListening(false);
                if (transcript) {
                  handleAskAI(transcript);
                }
              };

              newRecognition.onerror = (event) => {
                setIsListening(false);
                setError(`Speech recognition error: ${event.error}`);
                console.error('Speech recognition error:', event.error);
              };

              newRecognition.onend = () => {
                setIsListening(false);
              };

              setRecognition(newRecognition);
            } else {
              setError("Speech Recognition is not supported in this browser.");
            }
          }, []);

          // Function to start or stop listening
          const toggleListening = () => {
            if (isListening) {
              recognition.stop();
            } else {
              setSpeechText('');
              setAnswer('');
              recognition.start();
            }
          };

          // Function to call the Gemini API for an answer
          const handleAskAI = async (text) => {
            if (!text) return;
            setIsAiProcessing(true);
            setAnswer('');

            try {
              // The prompt is now structured to give a very concise answer
              const prompt = `Based on the following spoken question, provide a single, short, and quick answer: "${text}"`;
              
              const payload = {
                contents: [{
                  role: "user",
                  parts: [{ text: prompt }]
                }]
              };

              const apiKey = "";
              const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

              const response = await fetch(apiUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });
              
              if (!response.ok) {
                throw new Error(`API error: ${response.statusText}`);
              }

              const result = await response.json();
              
              if (result.candidates && result.candidates.length > 0 &&
                  result.candidates[0].content && result.candidates[0].content.parts &&
                  result.candidates[0].content.parts.length > 0) {
                const generatedText = result.candidates[0].content.parts[0].text;
                setAnswer(generatedText);
              } else {
                setAnswer("Sorry, I couldn't generate a response. Please try again.");
              }
            } catch (error) {
              console.error("Error asking AI:", error);
              setAnswer("An error occurred while fetching the answer.");
            } finally {
              setIsAiProcessing(false);
            }
          };

          return (
            <div className="min-h-screen bg-gradient-to-br from-purple-500 to-indigo-600 font-sans text-gray-100 p-8 flex flex-col items-center justify-center">
              <div className="bg-white/20 backdrop-blur-lg p-6 rounded-3xl shadow-2xl w-full max-w-sm space-y-6 border border-white/30 text-center">
                
                {/* Header Section */}
                <div className="mb-4">
                  <h1 className="text-3xl font-bold text-white mb-2">Voice AI Assistant</h1>
                  <p className="text-gray-200">Tap the mic, speak your question, and get an instant answer.</p>
                </div>

                {/* Microphone Button */}
                <button
                  onClick={toggleListening}
                  disabled={!recognition}
                  className={`w-28 h-28 rounded-full transition-all duration-300 shadow-lg 
                  ${isListening ? 'bg-red-500 transform scale-110 ring-4 ring-red-300' : 'bg-green-500 hover:bg-green-600 disabled:bg-gray-400'}`}
                >
                  <svg className={`mx-auto ${isListening ? 'animate-pulse' : ''}`} width="48" height="48" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12 14C14.21 14 16 12.21 16 10V4C16 1.79 14.21 0 12 0C9.79 0 8 1.79 8 4V10C8 12.21 9.79 14 12 14ZM10 4C10 2.9 10.9 2 12 2C13.1 2 14 2.9 14 4V10C14 11.1 13.1 12 12 12C10.9 12 10 11.1 10 10V4ZM12 18C15.31 18 18.06 15.69 18.06 12.5H20.06C20.06 16.3 17.15 19.45 13.5 19.96V23H10.5V19.96C6.85 19.45 3.94 16.3 3.94 12.5H5.94C5.94 15.69 8.69 18 12 18Z" fill="currentColor"/>
                  </svg>
                </button>

                {/* Status Messages */}
                <div className="h-6">
                  {error && <p className="text-red-300">{error}</p>}
                  {isListening && <p className="text-white animate-pulse">Listening...</p>}
                  {isAiProcessing && !isListening && <p className="text-white animate-pulse">Thinking...</p>}
                </div>

                {/* Output Section */}
                <div className="space-y-4">
                  <div className="bg-white/10 p-4 rounded-xl text-left min-h-[6rem] max-h-48 overflow-auto">
                    <h2 className="text-lg font-bold mb-2 text-white">Your Question:</h2>
                    <p className="text-gray-200">{speechText || <span className="italic text-gray-400">Your question will appear here.</span>}</p>
                  </div>

                  <div className="bg-white/10 p-4 rounded-xl text-left min-h-[6rem] max-h-48 overflow-auto">
                    <h2 className="text-lg font-bold mb-2 text-white">AI Answer:</h2>
                    <p className="text-gray-200">{answer || <span className="italic text-gray-400">The AI's answer will appear here.</span>}</p>
                  </div>
                </div>
              </div>
            </div>
          );
        }

        // Render the App component into the root element
        const rootElement = document.getElementById('root');
        ReactDOM.render(<App />, rootElement);
    </script>
</body>
</html>
